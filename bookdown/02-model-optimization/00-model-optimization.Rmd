# Model Optimization {#model-optim}

**Model Tuning**
Even though machine learning algorithms have default values set for their hyperparameters, these usually need to be set changed by the user to achieve optimal performance.
Selecting suitable parameter values manually is not an easy task.
This is why an automatic tuning of hyperparameters is preferred. 
Tuning hyperparameters implies automatic identification of values that lead to the best performance.
In order to tune a machine learning algorithm, you have to specify: 

- the search space,
- the optimization algorithm (aka tuning method) and 
- an evaluation method, i.e., a resampling strategy and a performance measure.

In summary, the sub-chapter on tuning illustrates hyperparameter selection, how to pick an optimizing algorithm and how to automate tuning.  

**Feature Selection** 
The second part of this chapter explains "feature selection". 
The objective of feature selection is to fit the sparse dependent of a model on a subset of available data features in the most suitable manner.
Feature selection can enhance the interpretability of the model, speed up the learning process and improve the learner performance.
Different approaches exist to identify the relevant features.
The chapter on feature selection introduces two distinct approaches to filtering, namely on the one hand "filtering" and on the other hand an approach referred to as "feature subset selection" or Wrapper methods".

**Nested Resampling**
The third section is about resampling strategies, specifically nested resampling. 
Resampling strategies are usually used to assess the performance of a learning algorithm: The entire data set is (repeatedly) split into training sets D∗b and test sets D∖D∗b, b=1,…,B.
Then the B individual performance values are aggregated.
Various different resampling strategies exist, for example cross-validation and bootstrap, to mention just two popular approaches.

